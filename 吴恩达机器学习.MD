# 监督机器学习   

样本的值是具体已知的。

1. 回归问题：根据数据集预测连续的数值
2. 分类问题：根据数据集预测离散的数值



# 无监督机器学习    

只有数据集没有任何标签(lable)与数值，自动找出数据的某种结构

 

#  2.线性模型

- 训练集：给出需要预测的数据集(x(i),y(i))
- 假设函数：h(x)=θ0+θ1x
- 代价（损失）函数：![](https://ftp.bmp.ovh/imgs/2021/02/360dcdff39fc6607.png)

#  3.梯度下降法 

- 用于**最小化**损失函数J，求解目标函数系数。局部最低点

- 梯度：目标函数的等高线图 增长最快的方向，是一个向量。

- ![](https://ftp.bmp.ovh/imgs/2021/02/5d67e5b9f7586f40.png)

- 初始化θ0，θ1，通常为0。

- 需要同步更新θ1，θ0

- α：学习效率，梯度下降更新参数的速率

- 由于J需要整个训练集的样本，因此每次梯度下降需要遍历整个样本

- 二元线性回归函数各个系数的偏导：

  ![4skcLV.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/4skcLV.png)

  

  - 线性函数梯度下降公式：

    ![4sACOP.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/4sACOP.png)OP)

  - 线性方程使用矩阵表示：

    ![4sZkTK.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/4sZkTK.png)

    - X * A = Y；

    - X: 行为不同维度的影响因子，每一行第一个通常为1，即常数项因子。列为数据组数。

    - A：系数矩阵：同一列为一组对应因子前的系数。

    -  矩阵求逆：AA* = |A|I，A*为伴随矩阵：每个元素的**代数余子式** 构成的矩阵的**转置**。

    - 代数余子式：M(i,j) = (-1）^(i + j)  * |划掉所在行，所在列剩余矩阵。

      

# 4.多功能线性回归

## 4.1多特征	

- n：特征维度

- ![4fC9HI.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/4fC9HI.png)

- 用矩阵乘法来表示hypothesis formula：

  ![4fiGnJ.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/4fiGnJ.png)



## 4.2多元梯度下降

- 多元损失函数J(θ)：
- ![4fAbEd.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/4fAbEd.png)
- 梯度下降中的偏导项：
  - ![4fVTkd.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/4fVTkd.png)



## 4.3特征缩放

- 当多个特征的范围一致时：梯度下降速度块：

  - ![4fmSWn.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/4fmSWn.png)

- 不一致时：速度慢。

  - 假设hypothesis有两个参数：θ1>>θ2 。损失函数的对应的等值(高）线:

    - [![4feUqU.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/4feUqU.png)

    - 根据梯度公式：

      ![4fVTkd.png](https://z3.ax1x.com/2021/09/28/4fVTkd.png)

    - 范围越大的哪个特质值，梯度中对应参数参数值越大。

- 通常特征缩放：将特征范围缩放在：-1<= x <= 1 

- 常用方法：均值归一化：

  - $$
    \begin{equation}
    	\begin{cases}
    	 \text{均值归一化:} \frac{x_i-min}{max - min} / \frac{x_i-u_i}{max- min} \\
    	 \text{标准化:} \frac{x_i - u_i}{S_i}
    	\end{cases}
    \end{equation}
    $$

    ![4fKSWd.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/4fKSWd.png)

  

  

## 4.4学习率：

- 学习率过大：反复横跳，不收敛。
  - 
- 小：梯度下降速度慢。
- 通常画出损失函数 J(θ) 与梯度下降递归次数的函数图像。应该是个单调递减图像。  
  - ![4fKfXt.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/4fKfXt.png)



## 4.5特征与多项式回归

- 多项式可以将高次项换元成一次项 变成线性回归。

  - ![4fKfXt.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/4fKfXt.png)

    

## 4.6正规方程

- 给定一组样本数据 与 预期结果，用矩阵表示为：
  - ![4fcsfg.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/4fcsfg.png)

  - 正规方程：

  - ![4zlEF0.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/4zlEF0.png)

  - 证明过程：

    [正规方程证明过程]: https://www.cnblogs.com/pjishu/p/10744641.html

  - 正规方程与梯度下降：

    -  梯度下降：
      - 需要选择学习效率α，需要迭代
      - n很大时仍然可以很好的运行
    - 正规方程
      - 不需要选择学习效率，也不需要迭代
      - n很大时不能很好的运行，计算矩阵的逆开销很大O(N^3)
      - 线性回归模型中，数据特征少时可以替代梯度下降

- 正规方程矩阵不可逆时（奇异矩阵\退化矩阵）：

  - 可能是有两列（不同特征）是线性相关的，那么就删除一个特征。
  - 可能是特征值过多，则删除一些特征值。







#  5.分类：逻辑回归算法



## 5.1 二分类问题

- logical function/sigmod function:
  - ![I55LLt.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/I55LLt.png)
  - 输出：二分类：
    - h(x) = P (y = 1| x;θ) 为一个条件概率
    - P( Y = 0 | x;θ) = 1 - P (y = 1| x;θ)
  
- 决定边界
  - θX
  - θX >=0,out>0.5
  - θX<0,out<0.5
  - 决策边界不是由数据集决定，给定θ即确定一个决策边界，数据集不断地拟合决策边界。
  
- 代价函数
  - ![I5b4ot.md.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/I5b4ot.md.png)
  
  - ![I5q3md.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/I5q3md.png)
  
  - 由于sigmod为非线性函数，代价函数不是凸函数，梯度下降会找到局部最优解。
  
  - 采用新cost_func
  
    - ![I5OJRf.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/I5OJRf.png)
    - ![I5Oriq.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/I5Oriq.png)
    - ![I5XFOg.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/I5XFOg.png)
  
  - 连续性cost_function：
  
    - ![I5X0XD.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/I5X0XD.png)
  
    
  
    - 极大似然估计法得出地cost_func![I5jCNR.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/I5jCNR.png)
  
  - 梯度下降算法：拟合θ
  
    - ![II9zzn.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/II9zzn.png)
  
  - 其他算法：
    - Conjugate gradient
    - BFGS
    - L-BFGS
    - 不需要手动选择学习效率

## 5.2 多分类问题

### 5.2.1一对多

- 使用多个二分类器函数:拟合出多个决定边界

​	![IT9SET.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/IT9SET.png)

- 预测：x特征值输入每个分类器，得到的最大值：

  ![IT9yq0.png](https://cdn.jsdelivr.net/gh/VincentVanNF/picgo/img/IT9yq0.png)



# 5.3 过拟合

##  5.3.1线性回归正则化

- 特征值过多，数据集过少
- 解决：
  - 减少特征向量
  - **正则化** （ 减少规模/或者参数θ的大小）
  - 正则化方程
  - [![ITghQI.png](https://z3.ax1x.com/2021/11/18/ITghQI.png)](https://imgtu.com/i/ITghQI)
    - 前一半为优化参数目标损失函数，后一半为正则化参数*参数平方之和，会使得每个θ都适当减小。
    - 正则化参数太大，使得惩罚系数过大，θ1到θj都趋近于0，而退化成水平直线。
  - 正则化后的梯度下降:
    - [![IT4omn.png](https://z3.ax1x.com/2021/11/18/IT4omn.png)](https://imgtu.com/i/IT4omn)
  - 正规方程使用正则化:
    - [![ITIm2F.png](https://z3.ax1x.com/2021/11/18/ITIm2F.png)](https://imgtu.com/i/ITIm2F)
    - 同时解决了X^T*X 矩阵不可逆的情况。

## 5.3.2 logical回归正则化

- 同理线性回归的损失函数：
  - [![IT7KKg.png](https://z3.ax1x.com/2021/11/18/IT7KKg.png)](https://imgtu.com/i/IT7KKg)
  - 梯度下降：
  - [![IT7bz8.png](https://z3.ax1x.com/2021/11/18/IT7bz8.png)](https://imgtu.com/i/IT7bz8)
  - 高级优化函数使用正则化:
    - 自定义costFunction函数计算损失，和梯度：
    - [![ITbitI.png](https://z3.ax1x.com/2021/11/18/ITbitI.png)](https://imgtu.com/i/ITbitI)

